
import sys
import torch
import numpy as np
from executorch.backends.xnnpack.partition.xnnpack_partitioner import XnnpackPartitioner
from executorch.exir import to_edge_transform_and_lower

def print_inline_every(iter, freq, term, msg):
    if iter % freq == 0 or iter == term - 1:
        if iter > 0: sys.stdout.write("\033[F\033[K")
        print(msg)

def tensor_to_image(tensor):
    image = tensor.cpu().clone()
    image = image.squeeze(0)
    image = image.permute(1, 2, 0) # C x H x W  -> H x W x C
    image = (image - image.min()) / (image.max() - image.min()) # Normalize to [0, 1]
    image = (image.numpy() * 255).astype('uint8')
    return image

def conditional_event(conditional_event_rate, random_number_generator):
    """
    Evaluate stochastic event based off of the conditional_event_rate.

    An event is represented by a 1 and no event a 0. For example if the
    conditional_event_rate = 0.7, that means that 70% of the time this function
    will return a 1 and 30% of the time a 0.

    Parameters
    ----------
    conditional_event_rate : float
        Rate at which the event occurs and a 1 is generated.
    random_number_generator : numpy.random._generator.Generator
        Random number generator used to generate uniform random variables.

    Returns
    -------
    error : int
        Either 0 or 1, 1 represents the event occurring.

    """
    event_draw = random_number_generator.uniform()

    if random_number_generator.uniform() < conditional_event_rate:
        event = 1
    else:
        event = 0

    return event

import time
def simulate_errors(data, p=None, r=None, k=1, h=0):
    '''
    Simulate errors using the Gilbert-Elliot burst error model.

    Creates a list representing an error signal. The error signal is expected
    to be added with modulo 2 addition to a binary signal, so 0 means no error
    occurred in that element and a 1 means an error did occur.

    Parameters
    ----------
    data : bytes
        The input byte stream.
    p : float
        Probability of transitioning from the Good state to the Bad state.
    r : float
        Probability of transitioning from the Bad state to the Good state.
    k : float
        Probability of no error occurring when in the Good state.
    h : float
        Probability of no error occurring when in the Bad state.

    Returns
    -------
    errors : list
        List of errors generated by the model, where 0 represents no error,
        and 1 represents an error.

    Notes
    -----
    - This implementation is derived from the original by NTIA [1].
    - For 2.4 GHz network, the following parameters are reasonable [2]:
        - Good Link: p=0.13, r=0.84
        - Mid Link: p=0.29, r=0.78
        - Bad Link: p=0.92, r=0.08

    [1] Pieper J; Voran S, Relationships between Gilbert-Elliot Burst Error Model Parameters and Error Statistics, NTIA Technical Memo TM-23-565.  
    [2] A. Bildea, O. Alphand, F. Rousseau and A. Duda, "Link quality estimation with the Gilbert-Elliot model for wireless sensor networks," 2015 IEEE 26th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC), Hong Kong, China, 2015, pp. 2049-2054, doi: 10.1109/PIMRC.2015.7343635.
    '''
    if p is None and r is None:
        rand = np.random.random_sample()
        if rand < 0.4:
            return data
        elif rand < 0.7:
            p, r = 0.13, 0.84
        elif rand < 0.9:
            p, r = 0.29, 0.78
        else:
            p, r = 0.92, 0.08

    n = len(data) * 8                           # Number of bits in the data
    rand = np.random.random_sample((n+1, 2))      # Generate random numbers for states and errors
    arr = np.zeros((n+1, 2), dtype=bool)        # Initialize array for states and errors
    arr[0, 0] = rand[0, 0] < p/(p + r)          # First state according to proportion of time in bad state
    _r, _k, _h = 1 - r, 1 - k, 1 - h            # Pre-invert probabilities

    # Choose next state and evaluate error for each step to generate error mask
    for i in np.arange(1, n):
        arr[i] = rand[i] < np.array([_r, _h] if arr[i-1][0] else [p, _k])

    # Evaluate last state and error
    arr[n, 1] = rand[n, 1] < (_h if arr[n-1][0] else _k)

    mask = np.packbits(arr[1:, 1])              # Pack error mask into bytes
    bits = np.frombuffer(data, dtype=np.uint8)  # Get data from the byte stream
    bits = np.bitwise_xor(bits, mask).tobytes() # XOR the error signal with the data
    return bits


def simulate_impairments(data: bytes, snr_db: float = 10, interference_prob: float = 0.1, flip_prob: float = 0.01, device: str = "cuda") -> bytes:
    """
    Apply realistic impairments (AWGN, fading, interference, burst errors) to a byte stream using PyTorch for GPU acceleration.
    
    Parameters
    ----------
    data : bytes
        The input byte stream.
    snr_db : float
        Signal-to-noise ratio in dB for AWGN.
    interference_prob : float
        Probability of random bit flips.
    flip_prob : float
        Probability of burst errors.
    device : str
        Device to run computations on ("cuda" or "cpu").
    
    Returns
    -------
    bytes
        The impaired byte stream.
    """
    # Move data to tensor on GPU
    bits = torch.from_numpy(np.unpackbits(np.frombuffer(data, dtype=np.uint8))).to(device, dtype=torch.float32)
    rand = np.random.rand(3)

    # # Simulate additive white Gaussian noise
    # if rand[0] < interference_prob:
    #     noise_std = torch.sqrt(torch.tensor(1 / (2 * 10 ** (snr_db / 10)), device=device))
    #     bits = bits + torch.randn_like(bits) * noise_std
    
    # # Simulate Rayleigh fading
    # if rand[1] < interference_prob:
    #     bits = bits * torch.abs(torch.randn_like(bits, device=device))
    
    # Simulate bit flips
    if rand[2] < interference_prob:
        flip_mask = torch.rand_like(bits, device=device) < flip_prob
        bits[flip_mask] = 1 - bits[flip_mask]

    bits = torch.sigmoid(bits).round().to(torch.uint8)
    
    # Move back to CPU, convert to bytes
    bits = np.packbits(bits.detach().cpu().numpy()).tobytes()
    return bits

def export_xnnpack(self, path):
    '''
    Export the model to XNNPACK format

    Parameters
    ----------
    path : str
        Path to save the model

    Notes
    -----
    Example usage of the exported XNNPACK model in C++:
    
    ```
    #include <executorch/extension/module/module.h>
    #include <executorch/extension/tensor/tensor.h>

    using namespace ::executorch::extension;

    // Load the model.
    Module module("/path/to/model.pte");

    // Create an input tensor.
    float input[1 * 3 * 480 * 640];
    auto tensor = from_blob(input, {1, 3, 480, 640});

    // Perform an inference.
    const auto result = module.forward(tensor);

    // Retrieve the output data.
    if (result.ok()) {
        const auto output = result->at(0).toTensor().const_data_ptr<float>();
    }
    ```
    '''
    self.eval()                                         # Set the model to evaluation mode
    inputs = torch.randn(self.batch_size, 3, 480, 640)  # Create a sample input tensor
    export = torch.export.export(self, inputs)          # Export the model
    partitioner = [XnnpackPartitioner()]                # Create a partitioner for XNNPACK

    # Lower the model, then transform the model to Executorch backend
    et_program = to_edge_transform_and_lower(           
        export, 
        partitioner=partitioner
    ).to_executorch()                                   

    # Save the model to the specified path
    with open(path, 'wb') as f:
        f.write(et_program.buffer)


# from executorch.runtime import Runtime

# def test_xnnpack(path: str, method: str, input: torch.Tensor):
#     '''
#     Test the exported XNNPACK model

#     Parameters
#     ----------
#     path : str
#         Path to the exported XNNPACK model
#     method : str
#         Method to be executed
#     input : torch.Tensor
#         Input tensor to the model | *(1, 3, 480, 640)*

#     Returns
#     -------
#     outputs : list
#         Output tensors from the model
#     '''
#     assert input.shape == (1, 3, 480, 640), f"Input shape must be (1, 3, 480, 640), got {input.shape}"
#     runtime = Runtime.get()                 # Get the runtime instance
#     program = runtime.load_program(path)    # Load the exported XNNPACK model
#     method = program.load_method(method)    # Load the specified method
#     outputs = method.execute([input])       # Execute the model with the input tensor
#     return outputs